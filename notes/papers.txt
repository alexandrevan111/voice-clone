Fitting New Speakers Based on a Short Untranscribed Sample
https://arxiv.org/pdf/1802.06984.pdf

Generalized end-to-end loss for speaker verification
https://arxiv.org/pdf/1710.10467.pdf

Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis
https://arxiv.org/pdf/1807.11470.pdf

VOICE IMPERSONATION USING GENERATIVE ADVERSARIAL NETWORKS
https://arxiv.org/pdf/1802.06840.pdf

Neural Style Transfer for Audio Spectrograms
https://arxiv.org/pdf/1801.01589.pdf

ON USING BACKPROPAGATION FOR SPEECH TEXTURE GENERATION AND VOICE CONVERSION
https://arxiv.org/pdf/1712.08363.pdf

Cross-modal Supervision for Learning Active Speaker Detection in Video
https://arxiv.org/pdf/1603.08907.pdf

VOICELOOP: VOICE FITTING AND SYNTHESIS VIA A PHONOLOGICAL LOOP
https://arxiv.org/pdf/1707.06588.pdf

DEEP VOICE 3: SCALING TEXT-TO-SPEECH WITH CONVOLUTIONAL SEQUENCE LEARNING
https://arxiv.org/pdf/1710.07654.pdf

SCALING AND BIAS CODES FOR MODELING SPEAKER-ADAPTIVE DNN-BASED SPEECH SYNTHESIS SYSTEMS
https://arxiv.org/pdf/1807.11632.pdf

Music Style Transfer: A Position Paper
https://arxiv.org/pdf/1803.06841.pdf


https://github.com/andabi/deep-voice-conversion






### Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis ###
URL: https://arxiv.org/pdf/1806.04558.pdf

CITES: 1802.06006, 1710.10467, 1707.06588

CITED BY: 1802.06984

SUMMARY: Study of a zero-shot voice cloning framework with three independently trained parts. A speaker encoder that derives embeddings from a voice sample, a synthesizer that generates an MFCC given a text input and speaker embedding, and a vocoder network that "inverts" the MFCC to retrieve an audio waveform. (...)

NOTES: 
- The datasets used are VCTK, LibriSpeech (same as 1802.06006), VoxCeleb 1 and 2 as well as some internal datasets.
- Many restrictions are lifted on the dataset due to the independent trainings.
- The samples are by far the most natural ones out of all other frameworks, and they only require ~4 seconds of speech! https://google.github.io/tacotron/publications/speaker_adaptation/
- This is the first paper I see that demonstrates transfer across languages. However they do not describe any specific approach to do so.
- "(...) the audio generated by our model for unseen speakers is deemed to be at least as natural
as that generated for same speakers."
- "The speaker encoder is trained only on North American accented speech", and thus it likely makes a poor standard for different languages.
- "Additionally, we note that in informal listening tests we found that the prosody of the synthesized
speech sometimes mimics that of the reference utterance."
- "The better generalization of the LibriSpeech [w.r.t. VCTK] model suggests that training the synthesizer on only 100 speakers is insufficient to enable high quality speaker transfer", so I need to work with large datasets.
- They used the same speaker verification model as in 1802.06006 to get a metric.
- 




### Neural Voice Cloning with a Few Samples ###
URL: https://arxiv.org/pdf/1802.06006.pdf

CITES: 1710.07654

CITED BY: 1806.04558, 1807.11632

SUMMARY: Study of two few-shot (unseen speaker, small sample size) voice cloning methods. In both case, the approach is to train a multi-speaker generative model that takes both a text and a speaker embedding as input during inference. The first approach, speaker adaptation, consists in fine-tuning a pretrained multi-speaker generative model where the embedding is a parameter to optimize. Possibly, the model alone remains constant and only the embedding is optimized. In the second approach however, the embedding is derived from a pretrained encoder and thus needs not be optimized at inference time. This encoding model is either trained jointly with the multi-speaker generative model or on its own by using embeddings extracted from another pretrained multi-speaker model in a supervised fashion. To assess their models, they develop a speaker verification model (a binary classifier that verifies if a given audio corresponds to a given speaker) and speaker classifier. The later is more reliable (they claim 100% accuracy), but can only classify speakers it has already seen. Their results are decent, they manage to generate nice audio samples with only a few input samples.

NOTES: 
- The datasets used are VCTK and LibriSpeech (same as 1806.04558)
- This is a rich comparison of different training methods to retrieve the embeddings and to generate the audios. I will probably get to try some of those and the results reported in this paper will be a good reference.
- Using a speaker classifier and a speaker verification model to evaluate their performance is an interesting approach, considering that the alternative would typically be MOS or using engineered metrics that do not truly capture how good the results sound. I haven't seen what they use for loss function for training, but if it's not already what they do it could be interesting to use these models as such.
- They talk about models similar to theirs for speaker diarization, which will be useful for later.
- As far as I understand, they refer to "cloning" stricly as the process of integrating the speaker information by training a model, but not actually generating cloned speech (which is then called inference or generation phase).
- I think Figure 3 is a tiny bit confusing because it starts at 100 iterations rather than 0, which makes it look like the whole model accuracy is initially 100% when actually it should be lower.
- I assume they use the number of iterations reported in footnote 6 for Table 1, which would explain the significant cloning time differences.
- It seems that the overall best approach is the whole model speaker adaptation. If training time is an issue (but even then they report it takes only a few minutes) then I can fall back to speaker encoding with fine-tuning, where training is a matter of seconds.
- Examples are available at https://audiodemos.github.io/. Cloned speech generated with as little as 1 sample often already sounds intelligible, albeit robotic. This robotic effect becomes less noticeable with 20 or more samples, but remains nonetheless (the speech quality does not reach that of a human even with 100 samples). The examples demonstrating embedding translation are not too impressive but seem to make sense.
- A third-party implementation is publicly available: https://github.com/Sharad24/Neural-Voice-Cloning-with-Few-Samples 
- This is a good quote: "While text carries linguistic information and controls the content of the generated speech, speaker representation captures speaker characteristics such as pitch range, speech rate and accent."
- I will have to read the paper where they describe their layers more in details: https://arxiv.org/pdf/1710.07654.pdf.
- In the conclusion they note that their results could be improved by using a WaveNet architecture.
- They add in the conclusion that the embeddings alone seems not to capture everything, and that some of the information remains in the generative model itself. Would it make sense to use small models for embeddings instead of vectors? The idea being to make them more expressive. Of course that makes it much more difficult to do all the fancy translations in latent space.
- They bring attention to the importance of the quality of the audio in the dataset (I don't think they're talking about the sampling rate but I'm not 100% sure) and to the deversity of the speakers. Having a lot of speakers with different tones/accents is important.
























